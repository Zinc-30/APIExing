Why	O
NumPy	O
instead	O
of	O
Python	O
lists	O
?	O

[	O
closed	O
]	O

Is	O
it	O
worth	O
my	O
learning	O
NumPy	O
?	O

I	O
have	O
approximately	O
100	O
financial	O
markets	O
series	O
,	O
and	O
I	O
am	O
going	O
to	O
create	O
a	O
cube	O
array	O
of	O
100x100x100	O
=	O
1	O
million	O
cells	O
.	O

I	O
will	O
be	O
regressing	O
(	O
3-variable	O
)	O
each	O
x	O
with	O
each	O
y	O
and	O
z	O
,	O
to	O
fill	O
the	O
array	O
with	O
standard	O
errors	O
.	O

I	O
have	O
heard	O
that	O
for	O
"	O
large	O
matrices	O
"	O
I	O
should	O
use	O
NumPy	O
as	O
opposed	O
to	O
Python	O
lists	O
,	O
for	O
performance	O
and	O
scalability	O
reasons	O
.	O

Thing	O
is	O
,	O
I	O
know	O
Python	O
lists	O
and	O
they	O
seem	O
to	O
work	O
for	O
me	O
.	O

Is	O
the	O
scale	O
of	O
the	O
above	O
problem	O
worth	O
moving	O
to	O
NumPy	O
?	O

What	O
if	O
I	O
had	O
1000	O
series	O
(	O
that	O
is	O
,	O
1	O
billion	O
floating	O
point	O
cells	O
in	O
the	O
cube	O
)	O
?	O

This	O
doesn't	O
answer	O
your	O
question	O
but	O
you	O
might	O
consider	O
asking	O
about	O
your	O
problem	O
on	O
stats	O
exchange	O
.	O

It	O
sounds	O
to	O
me	O
like	O
you	O
are	O
trying	O
to	O
do	O
something	O
the	O
hard	O
way	O
when	O
an	O
easier	O
solutions	O
might	O
exist	O
.	O

Generally	O
speaking	O
,	O
filling	O
a	O
large	O
cube	O
with	O
sparse	O
data	O
is	O
not	O
the	O
best	O
way	O
to	O
handle	O
these	O
kind	O
of	O
problems	O
.	O

what	O
a	O
Class-A	O
example	O
of	O
moderation	O
over-reach	O
to	O
close	O
one	O
of	O
the	O
most	O
popular	O
questions	O
on	O
Numpy	O
for	O
such	O
dubious	O
reasons	O
.	O

And	O
ironically	O
I	O
have	O
been	O
a	O
member	O
of	O
Stack	B-API
Overflow	O
for	O
longer	O
than	O
any	O
of	O
these	O
so-called	O
experts	O
.	O

Gitouttahere	O
jobsworths	O
...	O

Quite	O
common	O
this	O
kind	O
of	O
overreach	O
on	O
Stack	B-API
Overflow	O
IME	O
.	O

NumPy's	O
arrays	O
are	O
more	O
compact	O
than	O
Python	O
lists	O
--	O
a	O
list	O
of	O
lists	O
as	O
you	O
describe	O
,	O
in	O
Python	O
,	O
would	O
take	O
at	O
least	O
20	O
MB	O
or	O
so	O
,	O
while	O
a	O
NumPy	O
3D	O
array	O
with	O
single-precision	O
floats	O
in	O
the	O
cells	O
would	O
fit	O
in	O
4	O
MB	O
.	O

Access	O
in	O
reading	O
and	O
writing	O
items	O
is	O
also	O
faster	O
with	O
NumPy	O
.	O

Maybe	O
you	O
don't	O
care	O
that	O
much	O
for	O
just	O
a	O
million	O
cells	O
,	O
but	O
you	O
definitely	O
would	O
for	O
a	O
billion	O
cells	O
--	O
neither	O
approach	O
would	O
fit	O
in	O
a	O
32-bit	O
architecture	O
,	O
but	O
with	O
64-bit	O
builds	O
NumPy	O
would	O
get	O
away	O
with	O
4	O
GB	O
or	O
so	O
,	O
Python	O
alone	O
would	O
need	O
at	O
least	O
about	O
12	O
GB	O
(	O
lots	O
of	O
pointers	O
which	O
double	O
in	O
size	O
)	O
--	O
a	O
much	O
costlier	O
piece	O
of	O
hardware	O
!	O

The	O
difference	O
is	O
mostly	O
due	O
to	O
"	O
indirectness	O
"	O
--	O
a	O
Python	O
list	O
is	O
an	O
array	O
of	O
pointers	O
to	O
Python	O
objects	O
,	O
at	O
least	O
4	O
bytes	O
per	O
pointer	O
plus	O
16	O
bytes	O
for	O
even	O
the	O
smallest	O
Python	O
object	O
(	O
4	O
for	O
type	O
pointer	O
,	O
4	O
for	O
reference	O
count	O
,	O
4	O
for	O
value	O
--	O
and	O
the	O
memory	O
allocators	O
rounds	O
up	O
to	O
16	O
)	O
.	O

A	O
NumPy	O
array	O
is	O
an	O
array	O
of	O
uniform	O
values	O
--	O
single-precision	O
numbers	O
takes	O
4	O
bytes	O
each	O
,	O
double-precision	O
ones	O
,	O
8	O
bytes	O
.	O

Less	O
flexible	O
,	O
but	O
you	O
pay	O
substantially	O
for	O
the	O
flexibility	O
of	O
standard	O
Python	O
lists	O
!	O

Alex	O
-	O
always	O
the	O
good	O
answer	O
.	O

Thank	O
you	O
-	O
point	O
made	O
.	O

I'll	O
go	O
with	O
Numpy	O
for	O
scalability	O
and	O
indeed	O
for	O
efficiency	O
.	O

I'm	O
thinking	O
I'll	O
also	O
soon	O
be	O
needing	O
to	O
learn	O
parallel	O
programming	O
in	O
Python	O
,	O
and	O
invest	O
in	O
some	O
OpenCL	O
capable	O
hardware	O
;)	O

I've	O
been	O
trying	O
to	O
use	O
"	O
sys.getsizeof()	O
"	O
to	O
compare	O
the	O
size	O
of	O
Python	O
lists	O
and	O
NumPy	O
arrays	O
with	O
the	O
same	O
number	O
of	O
elements	O
and	O
it	O
doesn't	O
seem	O
to	O
indicate	O
that	O
the	O
NumPy	O
arrays	O
were	O
that	O
much	O
smaller	O
.	O

Is	O
this	O
the	O
case	O
or	O
is	O
sys.getsizeof()	O
having	O
issues	O
figuring	O
out	O
how	O
big	O
a	O
NumPy	O
array	O
is	O
?	O

@USER	O
`	O
getsizeof	O
`	O
isn't	O
reliable	O
.	O

The	O
documentation	O
clearly	O
states	O
that	O
:	O
Only	O
the	O
memory	O
consumption	O
directly	O
attributed	O
to	O
the	O
object	O
is	O
accounted	O
for	O
,	O
not	O
the	O
memory	O
consumption	O
of	O
objects	O
it	O
refers	O
to	O
.	O

This	O
means	O
that	O
if	O
you	O
have	O
nested	O
python	O
lists	O
the	O
size	O
of	O
the	O
elements	O
isn't	O
taken	O
into	O
account	O
.	O

`	O
getsizeof	O
`	O
on	O
a	O
list	O
only	O
tells	O
you	O
how	O
much	O
RAM	O
the	O
list	O
object	O
itself	O
consumes	O
and	O
the	O
RAM	O
consumed	O
by	O
the	O
pointers	O
in	O
its	O
data	O
array	O
,	O
it	O
doesn't	O
tell	O
you	O
how	O
much	O
RAM	O
is	O
consumed	O
by	O
the	O
objects	O
that	O
those	O
pointers	O
refer	O
to	O
.	O

@USER	O
,	O
could	O
you	O
please	O
let	O
me	O
know	O
where	O
are	O
you	O
getting	O
these	O
numbers	O
?	O

NumPy	O
is	O
not	O
just	O
more	O
efficient	O
;	O
it	O
is	O
also	O
more	O
convenient	O
.	O

You	O
get	O
a	O
lot	O
of	O
vector	O
and	O
matrix	O
operations	O
for	O
free	O
,	O
which	O
sometimes	O
allow	O
one	O
to	O
avoid	O
unnecessary	O
work	O
.	O

And	O
they	O
are	O
also	O
efficiently	O
implemented	O
.	O

For	O
example	O
,	O
you	O
could	O
read	O
your	O
cube	O
directly	O
from	O
a	O
file	O
into	O
an	O
array	O
:	O

Sum	O
along	O
the	O
second	O
dimension	O
:	O

Find	O
which	O
cells	O
are	O
above	O
a	O
threshold	O
:	O

Remove	O
every	O
even-indexed	O
slice	O
along	O
the	O
third	O
dimension	O
:	O

Also	O
,	O
many	O
useful	O
libraries	O
work	O
with	O
NumPy	O
arrays	O
.	O

For	O
example	O
,	O
statistical	O
analysis	O
and	O
visualization	O
libraries	O
.	O

Even	O
if	O
you	O
don't	O
have	O
performance	O
problems	O
,	O
learning	O
NumPy	O
is	O
worth	O
the	O
effort	O
.	O

Thanks	O
-	O
you	O
have	O
provided	O
another	O
good	O
reason	O
in	O
your	O
third	O
example	O
,	O
as	O
indeed	O
,	O
I	O
will	O
be	O
searching	O
the	O
matrix	O
for	O
cells	O
above	O
threshold	O
.	O

Moreover	O
,	O
I	O
was	O
loading	O
up	O
from	O
sqlLite	O
.	O

The	O
file	O
approach	O
will	O
be	O
much	O
more	O
efficient	O
.	O

Alex	O
mentioned	O
memory	O
efficiency	O
,	O
and	O
Roberto	O
mentions	O
convenience	O
,	O
and	O
these	O
are	O
both	O
good	O
points	O
.	O

For	O
a	O
few	O
more	O
ideas	O
,	O
I'll	O
mention	O
speed	O
and	O
functionality	O
.	O

Functionality	O
:	O
You	O
get	O
a	O
lot	O
built	O
in	O
with	O
NumPy	O
,	O
FFTs	O
,	O
convolutions	O
,	O
fast	O
searching	O
,	O
basic	O
statistics	O
,	O
linear	O
algebra	O
,	O
histograms	O
,	O
etc	O
.	O

And	O
really	O
,	O
who	O
can	O
live	O
without	O
FFTs	O
?	O

Speed	O
:	O
Here's	O
a	O
test	O
on	O
doing	O
a	O
sum	O
over	O
a	O
list	O
and	O
a	O
NumPy	O
array	O
,	O
showing	O
that	O
the	O
sum	O
on	O
the	O
NumPy	O
array	O
is	O
10x	O
faster	O
(	O
in	O
this	O
test	O
--	O
mileage	O
may	O
vary	O
)	O
.	O

which	O
on	O
my	O
systems	O
(	O
while	O
I'm	O
running	O
a	O
backup	O
)	O
gives	O
:	O

Here's	O
a	O
nice	O
answer	O
from	O
the	O
FAQ	O
on	O
the	O
scipy.org	O
website	O
:	O

What	O
advantages	O
do	O
NumPy	O
arrays	O
offer	O
over	O
(	O
nested	O
)	O
Python	O
lists	O
?	O

Python	O
s	O
lists	O
are	O
efficient	O
general-purpose	O
containers	O
.	O

They	O
support	O

(	O
fairly	O
)	O
efficient	O
insertion	O
,	O
deletion	O
,	O
appending	O
,	O
and	O
concatenation	O
,	O

and	O
Python	O
s	O
list	O
comprehensions	O
make	O
them	O
easy	O
to	O
construct	O
and	O

manipulate	O
.	O

However	O
,	O
they	O
have	O
certain	O
limitations	O
:	O
they	O
don	O
t	B-API
support	O

vectorized	O
operations	O
like	O
elementwise	O
addition	O
and	O
multiplication	O
,	O

and	O
the	O
fact	O
that	O
they	O
can	O
contain	O
objects	O
of	O
differing	O
types	O
mean	O

that	O
Python	O
must	O
store	O
type	O
information	O
for	O
every	O
element	O
,	O
and	O
must	O

execute	O
type	O
dispatching	O
code	O
when	O
operating	O
on	O
each	O
element	O
.	O

This	O

also	O
means	O
that	O
very	O
few	O
list	O
operations	O
can	O
be	O
carried	O
out	O
by	O

efficient	O
C	O
loops	O
each	O
iteration	O
would	O
require	O
type	O
checks	O
and	O
other	O

Python	O
API	O
bookkeeping	O
.	O

Note	O
also	O
that	O
there	O
is	O
support	O
for	O
timeseries	O
based	O
on	O
NumPy	O
in	O
the	O
timeseries	O
scikits	O
:	O

http://pytseries.sourceforge.net	O

For	O
regression	O
,	O
I	O
am	O
pretty	O
sure	O
NumPy	O
will	O
be	O
orders	O
of	O
magnitude	O
faster	O
and	O
more	O
convenient	O
than	O
lists	O
even	O
for	O
the	O
100^3	O
problem	O
.	O

Speed-wise	O
I'm	O
not	O
so	O
sure	O
of	O
.	O

Here	O
is	O
a	O
quick	O
example	O
:	O
I've	O
created	O
a	O
function	O
(	O
of	O
x	O
)	O
that	O
returns	O
a	O
list	O
of	O
prime	O
numbers	O
between	O
2	O
and	O
x	O
:	O

Regular	O
Python	O
function	O
using	O
lists	O
:	O

`	O
def	O
findprimeupto	O
(	O
x	O
):	O

primes	O
=	O
[	O
]	O

n_primes	O
=	O
[	O
]	O

for	O
i	O
in	O
range	O
(	O
2	O
,	O
x	O
):	O

if	O
not	O
(	O
i	O
in	O
n_primes	O
):	O

primes.append	O
(	O
i	O
)	O

n_primes.append	O
(	O
i	O
)	O

for	O
j	O
in	O
range	O
(	O
len	O
(	O
primes	O
)):	O

if	O
i	O
n_primes	O
[	O
j	O
]:	O

n_primes	O
[	O
j	O
]	O
+=	O
primes	O
[	O
j	O
]	O

return	O
primes	O

import	O
time	O

start_time	O
=	O
time.time()	O

findprimeupto	O
(	O
10000	O
)	O

print	O
(	O
"	O
---	O
%s	O
seconds	O
---	O
"	O
%	O
str	O
(	O
time.time()	O
-	O
start_time	O
))	O

`	O

and	O
C-like	O
Python	O
function	O
using	O
NumPy	O
arrays	O
:	O

`	O
import	O
numpy	O

def	O
findprimeupto	O
(	O
x	O
):	O

primes	O
=	O
numpy.array	O
(	O
numpy.zeros	O
(	O
x	O
)	O
,	O
dtype=	O
numpy.int32	O
)	O

n_primes	O
=	O
numpy.array	O
(	O
numpy.zeros	O
(	O
x	O
)	O
,	O
dtype=	O
numpy.int32	O
)	O

primeslen	O
=	O
0	O

for	O
i	O
in	O
range	O
(	O
2	O
,	O
x	O
):	O

flag	O
=	O
1	O

for	O
j	O
in	O
range	O
(	O
primeslen	O
):	O

if	O
n_primes	O
[	O
j	O
]	O
==	O
i	O
:	O

flag	O
=	O
0	O

break	O

if	O
flag	O
:	O

primes	O
[	O
primeslen	O
]	O
=	O
i	O

n_primes	O
[	O
primeslen	O
]	O
=	O
i	O

primeslen	O
+=	O
1	O

for	O
j	O
in	O
range	O
(	O
primeslen	O
):	O

if	O
i	O
n_primes	O
[	O
j	O
]:	O

n_primes	O
[	O
j	O
]	O
+=	O
primes	O
[	O
j	O
]	O

return	O
[	O
primeslen	O
,	O
primes	O
]	O

import	O
time	O

start_time	O
=	O
time.time()	O

result	O
=	O
findprimeupto	O
(	O
10000	O
)	O

#for	O
i	O
in	O
range	O
(	O
result	O
[	O
0	O
]):	O

#	O
print	O
(	O
'	O
{	O
:d	O
}	O
'	O
.format	B-API
(	O
result	O
[	O
1	O
]	O
[	O
i	O
])	O
,	O
end=	O
"")	O

print()	O

print	O
(	O
"	O
---	O
%s	O
seconds	O
---	O
"	O
%	O
str	O
(	O
time.time()	O
-	O
start_time	O
))	O

`	O

The	O
former	O
,	O
supposedly	O
slow	O
implementation	O
using	O
lists	O
,	O
is	O
executed	O
in	O
0.6	O
seconds	O
and	O
the	O
later	O
,	O
supposedly	O
fast	O
NumPy	O
implementation	O
,	O
is	O
needs	O
50	O
seconds	O
.	O

If	O
someone	O
can	O
point	O
out	O
why	O
I'd	O
greatly	O
appreciate	O
it	O
.	O

BTW	O
,	O
pure	O
C	O
program	O
which	O
is	O
more	O
or	O
less	O
a	O
copy	O
of	O
NumPy	O
version	O
of	O
the	O
function	O
executes	O
in	O
less	O
than	O
0.04	O
s	O
.	O

The	O
speed	O
of	O
C	O
is	O
even	O
more	O
obvious	O
with	O
large	O
x	O
:	O

NumPy's	O
speed	O
depends	O
on	O
making	O
good	O
use	O
of	O
NumPy's	O
capabilities	O
for	O
vectorized	O
operations	O
.	O

You	O
need	O
to	O
rewrite	O
your	O
NumPy	O
example	O
in	O
a	O
more	O
NumPy-friendly	O
style	O
,	O
replacing	O
Python-level	O
for	O
loops	O
with	O
NumPy	O
vectorized	O
operations	O
.	O

You	O
can't	O
just	O
replace	O
lists	O
with	O
NumPy	O
arrays	O
and	O
expect	O
code	O
to	O
run	O
faster	O
.	O

I'm	O
trying	O
to	O
find	O
appropriate	O
changes	O
to	O
the	O
code	O
using	O
native	O
numpy	O
functions	O
,	O
but	O
this	O
seems	O
to	O
be	O
particularly	O
difficult	O
case	O
as	O
it	O
differs	O
significantly	O
from	O
standard	O
linear	O
algebra	O
problems	O
numpy	O
is	O
well	O
suited	O
for	O
.	O

I	O
suspect	O
numpy	O
array	O
access	O
functions	O
are	O
slow	O
,	O
in	O
fact	O
I've	O
just	O
posted	O
a	O
test	O
in	O
another	O
thread	O
that	O
demonstrates	O
that	O
,	O
but	O
I	O
don't	O
see	O
how	O
I	O
can	O
get	O
around	O
it	O
.	O

There	O
is	O
one	O
other	O
thing	O
.	O

If	O
I	O
define	O
arrays	O
of	O
floats	O
instead	O
of	O
integers	O
,	O
namely	O
if	O
I	O
change	O
declarations	O
of	O
primes	O
and	O
n_primes	O
to	O
:	O
primes	O
=	O
numpy.zeros	O
(	O
x	O
)	O
n_primes	O
=	O
numpy.zeros	O
(	O
x	O
)	O
then	O
runtime	O
drops	O
from	O
50s	O
to	O
9s	O
.	O

This	O
would	O
indicate	O
issues	O
with	O
typecasting	O
(	O
i.e.	O
all	O
numpy	O
functions	O
assume	O
arguments	O
are	O
type	O
float	O
)	O
which	O
indeed	O
could	O
gobble	O
up	O
time	O
.	O

All	O
in	O
all	O
numpy	O
is	O
not	O
well	O
suited	O
for	O
general	O
purpose	O
integer	O
arrays	O
.	O

Shame	O
though	O
.	O

Looking	O
at	O
your	O
code	O
there	O
are	O
just	O
a	O
few	O
thing	O
you	O
could	O
change	O
to	O
make	O
it	O
incredibly	O
fast	O
.	O

First	O
off	O
,	O
numpy	O
does	O
just	O
fine	O
with	O
integers	O
but	O
you	O
told	O
it	O
to	O
start	O
out	O
with	O
floats	O
then	O
change	O
them	O
to	O
ints	O
,	O
instead	O
use	O
`	O
zeros	O
(	O
x	O
,	O
dtype=int32	O
)`	O
(	O
or	O
even	O
better	O
:	O
`	O
empty	O
`)	O
.	O

Next	O
,	O
array	O
lookups	O
are	O
fast	O
in	O
Numpy	O
,	O
however	O
looping	O
in	O
Python	O
is	O
slow	O
.	O

Your	O
first	O
Inner	O
loop	O
can	O
be	O
replaced	O
with	O
a	O
single	O
vectorized	O
line	O
`	O
flag	O
=	O
(	O
n_primes	O
[:	O
primeslen	O
]=	O
=i	O
)	O
.any()	B-API
`	O
.	O

I	O
am	O
sure	O
your	O
other	O
inner	O
loop	O
could	O
be	O
vectorized	O
as	O
well	O
.	O

This	O
should	O
be	O
asked	O
as	O
a	O
separate	O
question	O

