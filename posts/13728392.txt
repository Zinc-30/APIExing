Moving average or running mean
Is there a scipy function or numpy function or module for python that calculates the running mean of a 1D array given a specific window ?
/ M
For a short , fast solution that does the whole thing in one loop , without dependencies , the code below works great .
UP D: more efficient solutions have been proposed by Alleo and jasaarim .
You can use ` np.convolve ` for that :
The ` mode ` argument specifies how to handle the edges . I chose the ` valid ` mode here because I think that's how most people expect running mean to work , but you may have other priorities . Here is a plot that illustrates the difference between the modes :
I like this solution because it is clean ( one line ) and relatively efficient ( work done inside numpy ) . But Alleo's " Efficient solution " using ` numpy.cumsum ` has better complexity .
Efficient solution
Convolution is much better than straightforward approach , but ( I guess ) it uses FFT and thus quite slow . However specially for computing the running mean the following approach works fine
The code to check
Note that ` numpy.allclose ( result1 , result2 )` is ` True ` , two methods are equivalent .
The greater N , the greater difference in time .
Nice solution ! My hunch is ` numpy.convolve ` is O ( mn ); its docs mention that ` scipy.signal.fftconvolve ` uses FFT .
This method does not deal with the edges of the array , does it ?
I thought that calling ` numpy.ones ` every time , like you do , would cause quite some overhead and dominate the comparison ( in particular for high ` N `) , but that seems not to be the case . On my machine , I was unable to measure any difference at all and ` running_mean ` is still orders of magnitude faster .
Actually , the function doest work for python 3.5 when passing it a np array 1-d
This didnt work for me with py3.5 ( ` cannot perform accumulate with flexible type `)
You can calculate a running mean with :
But it's slow .
Fortunately , numpy includes a convolve function which we can use to speed things up . The running mean is equivalent to convolving ` x ` with a vector that is ` N ` long , with all members equal to ` 1 / N ` . The numpy implementation of convolve includes the starting transient , so you have to remove the first N-1 points :
On my machine , the fast version is 20-30 times faster , depending on the length of the input vector and size of the averaging window .
Note that convolve does include a `' same '` mode which seems like it should address the starting transient issue , but it splits it between the beginning and end .
Note that removing the first N-1 points still leaves a boundary effect in the last points . An easier way to solve the issue is to use ` mode= ' valid '` in ` convolve ` which doesn't require any post-processing .
@USER - ` mode= ' valid '` removes the transient from both ends , right ? If ` len ( x )= 10 ` and ` N=4 ` , for a running mean I would want 10 results but ` valid ` returns 7 .
It removes the transient from the end , and the beginning doesn't have one . Well , I guess it's a matter of priorities , I don't need the same number of results on the expense of getting a slope towards zero that isn't there in the data . BTW , here is a command to show the difference between the modes : ` modes = ( ' full ' , ' same ' , ' valid ') ; [ plot ( convolve ( ones (( 200 , )) , ones (( 50 , )) / 50 , mode=m )) for m in modes ]; axis ([ -10 , 251 , - .1 , 1.1 ]); legend ( modes , loc= ' lower center ')` ( with pyplot and numpy imported ) .
@USER - very nice . You should make this an answer , I would certainly give it a vote .
pandas is more suitable for this than NumPy or SciPy . Its function rolling_mean does the job conveniently . It also returns a NumPy array when the input is an array .
It is difficult to beat ` rolling_mean ` in performance with any custom pure Python implementation . Here is an example performance against two of the proposed solutions :
There are also nice options as to how to deal with the edge values .
The Pandas rolling_mean is a nice tool for the job but has been deprecated for ndarrays . In future Pandas releases it will only function on Pandas series . Where do we turn now for non-Pandas array data ?
@USER rolling_mean() is deprecated , but now you can use rolling and mean separately : ` df.rolling ( windowsize ) .mean() ` now works instead ( very quickly I might add ) . for 6,000 row series ` %timeit test1.rolling ( 20 ) .mean() ` returned 1000 loops , best of 3 : 1.16 ms per loop
@USER ` df.rolling() ` works well enough , the problem is that even this form will not support ndarrays in the future . To use it we will have to load our data into a Pandas Dataframe first . I would love to see this function added to either ` numpy ` or ` scipy.signal ` .
@USER totally agree . I am struggling in particular to match pandas .ewm() .mean() speed for my own arrays ( instead of having to load them into a df first ) . I mean , it's great that it's fast , but just feels a bit clunky moving in and out of dataframes too often .
` %timeit bottleneck.move_mean ( x , N )` is 3 to 15 times faster than the cumsum and pandas methods on my pc . Take a look at their benchmark in the repo's README .
For a ready-to-use solution , see http://www.scipy.org/Cookbook/SignalSmooth .
It provides running average with the ` flat ` window type . Note that this is a bit more sophisticated than the simple do-it-yourself convolve-method , since it tries to handle the problems at the beginning and the end of the data by reflecting it ( which may or may not work in your case ... ) .
To start with , you could try :
This method relies on ` numpy.convolve ` , the difference only in altering the sequence .
I'm always annoyed by signal processing function that return output signals of different shape than the input signals when both inputs and outputs are of the same nature ( e.g. , both temporal signals ) . It breaks the correspondence with related independent variable ( e.g. , time , frequency ) making plotting or comparison not a direct matter ... anyway , if you share the feeling , you might want to change the last lines of the proposed function as y= np.convolve ( w / w.sum() , s , mode= ' same ') ; return y [ window_len-1 :-( window_len-1 )]
or module for python that calculates
in my tests at Tradewave.net TA-lib always wins :
results :
I know this is an old question , but here is a solution that doesn't use any extra data structures or libraries . It is linear in the number of elements of the input list and I cannot think of any other way to make it more efficient ( actually if anyone knows of a better way to allocate the result , please let me know ) .
NOTE : this would be much faster using a numpy array instead of a list , but I wanted to eliminate all dependencies . It would also be possible to improve performance by multi-threaded execution
The function assumes that the input list is one dimensional , so be careful .
I haven't yet checked how fast this is , but you could try :
This is what I was going to do . Can anyone please critique why this is a bad way to go ?
This simple python solution worked well for me without requiring numpy . I ended up rolling it into a class for re-use .
A bit late to the party , but I've made my own little function that does NOT wrap around the ends or pads with zeroes that are then used to find the average as well . As a further treat is , that it also re-samples the signal at linearly spaced points . Customize the code at will to get other features .
The method is a simple matrix multiplication with a normalized Gaussian kernel .
A simple usage on a sinusoidal signal with added normal distributed noise :
Another approach to find moving average without using numpy , panda
will print [ 2.0 , 4.0 , 6.0 , 6.5 , 7.4 , 7.833333333333333 ]
itertools.accumulate does not exist in python 2.7 , but does in python 3.4
This question is now even older than when NeXuS wrote about it last month , BUT I like how his code deals with edge cases . However , because it is a " simple moving average , " it's results lag behind the data they apply to . I thought that dealing with edge cases in a more satisfying way than NumPy's modes ` valid ` , ` same ` , and ` full ` could be achieved by applying a similar approach to a ` convolution() ` based method .
My contribution uses a central running average to align its results with their data . When there are two few points available for the full-sized window to be used , running averages are computed from successively smaller windows at the edges of the array . [ Actually , from successively larger windows , but that's an implementation detail . ]
It's relatively slow because it uses ` convolve() ` , and could likely be spruced up quite a lot by a true Pythonista , however , I believe that the idea stands .
If it is important to keep the dimensions of the input ( instead of restricting the output to the `' valid '` area of a convolution ) , you can use scipy.ndimage.filters.uniform_filter1d :
` uniform_filter1d ` allows multiple ways to handle the border where `' reflect '` is the default , but in my case , I rather wanted `' nearest '` .
It is also rather quick ( nearly 50 times faster than ` np.convolve `) :
If you do choose to roll your own , rather than use an existing library , please be conscious of floating point error and try to minimize its effects :
If all your values are roughly the same order of magnitude , then this will help to preserve precision by always adding values of roughly similar magnitudes .
This is a terribly unclear answer , at least some comment in the code or explanation of why this helps floating point error would be nice .
In my last sentence I was trying to indicate why it helps floating point error . If two values are approximately the same order of magnitude , then adding them loses less precision than if you added a very large number to a very small one . The code combines " adjacent " values in a manner that even intermediate sums should always be reasonably close in magnitude , to minimize the floating point error . Nothing is fool proof but this method has saved a couple very poorly implemented projects in production .
1 . being applied to original problem , this would be terribly slow ( computing average ) , so this is just irrelevant 2 . to suffer from the problem of precision of 64-bit numbers , one has to sum up 2^30 of nearly equal numbers .
@USER : Instead of doing one addition per value , you'll be doing two . The proof is the same as the bit-flipping problem . However , the point of this answer is not necessarily performance , but precision . Memory usage for averaging 64-bit values would not exceed 64 elements in the cache , so it's friendly in memory usage as well .
Yes , you're right that this takes 2x more operations than simple sum , but the original problem is compute running mean , not just sum . Which can be done in O ( n ) , but your answer requires O ( mn ) , where m is size of window .