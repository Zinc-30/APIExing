Why NumPy instead of Python lists ? [ closed ]
Is it worth my learning NumPy ?
I have approximately 100 financial markets series , and I am going to create a cube array of 100x100x100 = 1 million cells . I will be regressing ( 3-variable ) each x with each y and z , to fill the array with standard errors .
I have heard that for " large matrices " I should use NumPy as opposed to Python lists , for performance and scalability reasons . Thing is , I know Python lists and they seem to work for me .
Is the scale of the above problem worth moving to NumPy ?
What if I had 1000 series ( that is , 1 billion floating point cells in the cube ) ?
This doesn't answer your question but you might consider asking about your problem on stats exchange . It sounds to me like you are trying to do something the hard way when an easier solutions might exist . Generally speaking , filling a large cube with sparse data is not the best way to handle these kind of problems .
what a Class-A example of moderation over-reach to close one of the most popular questions on Numpy for such dubious reasons . And ironically I have been a member of Stack Overflow for longer than any of these so-called experts . Gitouttahere jobsworths ...
Quite common this kind of overreach on Stack Overflow IME .
NumPy's arrays are more compact than Python lists -- a list of lists as you describe , in Python , would take at least 20 MB or so , while a NumPy 3D array with single-precision floats in the cells would fit in 4 MB . Access in reading and writing items is also faster with NumPy .
Maybe you don't care that much for just a million cells , but you definitely would for a billion cells -- neither approach would fit in a 32-bit architecture , but with 64-bit builds NumPy would get away with 4 GB or so , Python alone would need at least about 12 GB ( lots of pointers which double in size ) -- a much costlier piece of hardware !
The difference is mostly due to " indirectness " -- a Python list is an array of pointers to Python objects , at least 4 bytes per pointer plus 16 bytes for even the smallest Python object ( 4 for type pointer , 4 for reference count , 4 for value -- and the memory allocators rounds up to 16 ) . A NumPy array is an array of uniform values -- single-precision numbers takes 4 bytes each , double-precision ones , 8 bytes . Less flexible , but you pay substantially for the flexibility of standard Python lists !
Alex - always the good answer . Thank you - point made . I'll go with Numpy for scalability and indeed for efficiency . I'm thinking I'll also soon be needing to learn parallel programming in Python , and invest in some OpenCL capable hardware ;)
I've been trying to use " sys.getsizeof() " to compare the size of Python lists and NumPy arrays with the same number of elements and it doesn't seem to indicate that the NumPy arrays were that much smaller . Is this the case or is sys.getsizeof() having issues figuring out how big a NumPy array is ?
@USER ` getsizeof ` isn't reliable . The documentation clearly states that : Only the memory consumption directly attributed to the object is accounted for , not the memory consumption of objects it refers to . This means that if you have nested python lists the size of the elements isn't taken into account .
` getsizeof ` on a list only tells you how much RAM the list object itself consumes and the RAM consumed by the pointers in its data array , it doesn't tell you how much RAM is consumed by the objects that those pointers refer to .
@USER , could you please let me know where are you getting these numbers ?
NumPy is not just more efficient ; it is also more convenient . You get a lot of vector and matrix operations for free , which sometimes allow one to avoid unnecessary work . And they are also efficiently implemented .
For example , you could read your cube directly from a file into an array :
Sum along the second dimension :
Find which cells are above a threshold :
Remove every even-indexed slice along the third dimension :
Also , many useful libraries work with NumPy arrays . For example , statistical analysis and visualization libraries .
Even if you don't have performance problems , learning NumPy is worth the effort .
Thanks - you have provided another good reason in your third example , as indeed , I will be searching the matrix for cells above threshold . Moreover , I was loading up from sqlLite . The file approach will be much more efficient .
Alex mentioned memory efficiency , and Roberto mentions convenience , and these are both good points . For a few more ideas , I'll mention speed and functionality .
Functionality : You get a lot built in with NumPy , FFTs , convolutions , fast searching , basic statistics , linear algebra , histograms , etc . And really , who can live without FFTs ?
Speed : Here's a test on doing a sum over a list and a NumPy array , showing that the sum on the NumPy array is 10x faster ( in this test -- mileage may vary ) .
which on my systems ( while I'm running a backup ) gives :
Here's a nice answer from the FAQ on the scipy.org website :
What advantages do NumPy arrays offer over ( nested ) Python lists ?
Python s lists are efficient general-purpose containers . They support
( fairly ) efficient insertion , deletion , appending , and concatenation ,
and Python s list comprehensions make them easy to construct and
manipulate . However , they have certain limitations : they don t support
vectorized operations like elementwise addition and multiplication ,
and the fact that they can contain objects of differing types mean
that Python must store type information for every element , and must
execute type dispatching code when operating on each element . This
also means that very few list operations can be carried out by
efficient C loops each iteration would require type checks and other
Python API bookkeeping .
Note also that there is support for timeseries based on NumPy in the timeseries scikits :
http://pytseries.sourceforge.net
For regression , I am pretty sure NumPy will be orders of magnitude faster and more convenient than lists even for the 100^3 problem .
Speed-wise I'm not so sure of . Here is a quick example : I've created a function ( of x ) that returns a list of prime numbers between 2 and x :
Regular Python function using lists :
` def findprimeupto ( x ):
primes = [ ]
n_primes = [ ]
for i in range ( 2 , x ):
if not ( i in n_primes ):
primes.append ( i )
n_primes.append ( i )
for j in range ( len ( primes )):
if i n_primes [ j ]:
n_primes [ j ] += primes [ j ]
return primes
import time
start_time = time.time()
findprimeupto ( 10000 )
print ( " --- %s seconds --- " % str ( time.time() - start_time ))
`
and C-like Python function using NumPy arrays :
` import numpy
def findprimeupto ( x ):
primes = numpy.array ( numpy.zeros ( x ) , dtype= numpy.int32 )
n_primes = numpy.array ( numpy.zeros ( x ) , dtype= numpy.int32 )
primeslen = 0
for i in range ( 2 , x ):
flag = 1
for j in range ( primeslen ):
if n_primes [ j ] == i :
flag = 0
break
if flag :
primes [ primeslen ] = i
n_primes [ primeslen ] = i
primeslen += 1
for j in range ( primeslen ):
if i n_primes [ j ]:
n_primes [ j ] += primes [ j ]
return [ primeslen , primes ]
import time
start_time = time.time()
result = findprimeupto ( 10000 )
#for i in range ( result [ 0 ]):
# print ( ' { :d } ' .format ( result [ 1 ] [ i ]) , end= "")
print()
print ( " --- %s seconds --- " % str ( time.time() - start_time ))
`
The former , supposedly slow implementation using lists , is executed in 0.6 seconds and the later , supposedly fast NumPy implementation , is needs 50 seconds . If someone can point out why I'd greatly appreciate it .
BTW , pure C program which is more or less a copy of NumPy version of the function executes in less than 0.04 s . The speed of C is even more obvious with large x :
NumPy's speed depends on making good use of NumPy's capabilities for vectorized operations . You need to rewrite your NumPy example in a more NumPy-friendly style , replacing Python-level for loops with NumPy vectorized operations . You can't just replace lists with NumPy arrays and expect code to run faster .
I'm trying to find appropriate changes to the code using native numpy functions , but this seems to be particularly difficult case as it differs significantly from standard linear algebra problems numpy is well suited for . I suspect numpy array access functions are slow , in fact I've just posted a test in another thread that demonstrates that , but I don't see how I can get around it .
There is one other thing . If I define arrays of floats instead of integers , namely if I change declarations of primes and n_primes to : primes = numpy.zeros ( x ) n_primes = numpy.zeros ( x ) then runtime drops from 50s to 9s . This would indicate issues with typecasting ( i.e. all numpy functions assume arguments are type float ) which indeed could gobble up time . All in all numpy is not well suited for general purpose integer arrays . Shame though .
Looking at your code there are just a few thing you could change to make it incredibly fast . First off , numpy does just fine with integers but you told it to start out with floats then change them to ints , instead use ` zeros ( x , dtype=int32 )` ( or even better : ` empty `) . Next , array lookups are fast in Numpy , however looping in Python is slow . Your first Inner loop can be replaced with a single vectorized line ` flag = ( n_primes [: primeslen ]= =i ) .any() ` . I am sure your other inner loop could be vectorized as well .
This should be asked as a separate question