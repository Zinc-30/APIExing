pandas : How do I split text in a column into multiple rows ?
I'm working with a large csv file and the next to last column has a string of text that I want to split by a specific delimiter . I was wondering if there is a simple way to do this using pandas or python ?
I want to split by the space ` ( ' ')` and then the colon ` ( ' : ')` in the ` Seatblocks ` column , but each cell would result in a different number of columns . I have a function to rearrange the columns so the ` Seatblocks ` column is at the end of the sheet , but I'm not sure what to do from there . I can do it in excel with the built in ` text-to-columns ` function and a quick macro , but my dataset has too many records for excel to handle .
Ultimately , I want to take records such John Lennon's and create multiple lines , with the info from each set of seats on a separate line .
This splits the Seatblocks by space and gives each its own row .
Or , to give each colon-separated string in its own column :
This is a little ugly , but maybe someone will chime in with a prettier solution .
@USER give an index to the Series when you apply ; they will become column names
While this answers the question , it is worth mentioning that ( probably ) split() creates a list for each row , which blows up the size of the ` DataFrame ` very quickly . In my case , running the code on a ~200M table resulted in ~10G memory ( +swap ... ) usage .
Though I am not sure it is because of ` split() ` , because simply ` reduce() `' ing through the column works like a charm . The problem then may lie in ` stack() ` ...
I am getting the error ` NameError : name ' Series ' is not defined ` for this . where is ` Series ` supposed to come from ? EDIT : nevermind , it should be ` pandas.Series ` since it is referring to the item from ` pandas `
Yep , @USER . I ` from pandas import Series ` for convenience / brevity .
Differently from Dan , I consider his answer quite elegant ... but unfortunately it is also very very inefficient . So , since the question mentioned " a large csv file " , let me suggest to try in a shell Dan's solution :
... compared to this alternative :
... and this :
The second simply refrains from allocating 100 000 Series , and this is enough to make it around 10 times faster . But the third solution , which somewhat ironically wastes a lot of calls to str.split() ( it is called once per column per row , so three times more than for the others two solutions ) , is around 40 times faster than the first , because it even avoids to instance the 100 000 lists . And yes , it is certainly a little ugly ...
EDIT : this answer suggests how to use " to_list() " and to avoid the need for a lambda . The result is something like
which is even more efficient than the third solution , and certainly much more elegant .
EDIT : the even simpler
works too , and is almost as efficient .
I'm having a little trouble with the amount of memory that this method consumes and I'm wondering if you could give me a little advice . I have a DataFrame that contains about 8000 rows , each with a string containing 9216 space delimited 8-b it integers . This is roughly 75MB , but when I apply the last solution verbatim , Python eats 2GB of my memory . Can you point me in the direction of some source that would tell me why this is , and what I can do to get around it ? Thanks .
You have a lot of lists and very small strings , which is more or less the worst case for memory usage in python ( and the intermediate step " .split() .tolist() " produces pure python objects ) . What I would probably do in your place would be to dump the DataFrame to a file , and then open it as csv with read_csv ( ..., sep= ' ') . But to stay on topic : the first solution ( together with the third , which however should be awfully slow ) may be the one offering you the lowest memory usage among the 4 , since you have a relatively small number of relatively long rows .
Hey Pietro , I tried your suggestion of saving to a file and re-loading , an it worked quite well . I ran into some trouble when I tried to do this in a StringIO object , and a nice solution to my problem has been posted here .
Your last suggestion of ` tolist() ` is perfect . In my case I only wanted one of the pieces of data in the list and was able to directly add a single column to my existing df by using .ix : ` df [ ' newCol '] = pd.DataFrame ( df.col.str.split() .tolist() ) .ix [: , 2 ]`
Ahh , I was having trouble getting this to work at first - something about ` obect of type ' float ' has no len() ` which was baffling , until I realized some of my rows had ` NaN ` in them , as opposed to ` str ` .
Another similar solution with chaining is use ` reset_index ` and ` rename ` :
If in column are NOT ` NaN ` values , the fastest solution is use ` list ` comprehension with ` DataFrame ` constructor :
But if column contains ` NaN ` only works ` str.split ` with parameter ` expand=True ` which return ` DataFrame ` ( documentation ) , and it explain why it is slowier :
Maybe it's worth mentioning that you necessarily need the ` expand=True ` option working with ` pandas.DataFrames ` while using ` .str .split() ` for example .
@USER - thank you for comment , I add it to answer .