Using	O
strides	B-API
for	O
an	O
efficient	O
moving	O
average	O
filter	O

I	O
recently	O
learned	O
about	O
strides	B-API
in	O
the	O
answer	O
to	O
this	O
post	O
,	O
and	O
was	O
wondering	O
how	O
I	O
could	O
use	O
them	O
to	O
compute	O
a	O
moving	O
average	O
filter	O
more	O
efficiently	O
than	O
what	O
I	O
proposed	O
in	O
this	O
post	O
(	O
using	O
convolution	O
filters	O
)	O
.	O

This	O
is	O
what	O
I	O
have	O
so	O
far	O
.	O

It	O
takes	O
a	O
view	O
of	O
the	O
original	O
array	O
then	O
rolls	O
it	O
by	O
the	O
necessary	O
amount	O
and	O
sums	O
the	O
kernel	O
values	O
to	O
compute	O
the	O
average	O
.	O

I	O
am	O
aware	O
that	O
the	O
edges	O
are	O
not	O
handled	O
correctly	O
,	O
but	O
I	O
can	O
take	O
care	O
of	O
that	O
afterward	O
...	O

Is	O
there	O
a	O
better	O
and	O
faster	O
way	O
?	O

The	O
objective	O
is	O
to	O
filter	O
large	O
floating	O
point	O
arrays	O
up	O
to	O
5000x5000	O
x	O
16	O
layers	O
in	O
size	O
,	O
a	O
task	O
that	O
`	O
scipy.ndimage.filters.convolve	O
`	O
is	O
fairly	O
slow	O
at	O
.	O

Note	O
that	O
I	O
am	O
looking	O
for	O
8-neighbour	O
connectivity	O
,	O
that	O
is	O
a	O
3x3	O
filter	O
takes	O
the	O
average	O
of	O
9	O
pixels	O
(	O
8	O
around	O
the	O
focal	O
pixel	O
)	O
and	O
assigns	O
that	O
value	O
to	O
the	O
pixel	O
in	O
the	O
new	O
image	O
.	O

EDIT	O
Clarification	O
on	O
how	O
I	O
see	O
this	O
working	O
:	O

Current	O
code	O
:	O

use	O
stride_tricks	O
to	O
generate	O
an	O
array	O
like	O
[[	O
0	O
,	O
1	O
,	O
2	O
]	O
,	O
[	O
1	O
,	O
2	O
,	O
3	O
]	O
,	O
[	O
2	O
,	O
3	O
,	O
4	O
]	O
...	O
]	O
which	O
corresponds	O
to	O
the	O
top	O
row	O
of	O
the	O
filter	O
kernel	O
.	O

Roll	O
along	O
the	O
vertical	O
axis	O
to	O
get	O
the	O
middle	O
row	O
of	O
the	O
kernel	O
[[	O
10	O
,	O
11	O
,	O
12	O
]	O
,	O
[	O
11	O
,	O
12	O
,	O
13	O
]	O
,	O
[	O
13	O
,	O
14	O
,	O
15	O
]	O
...	O
]	O
and	O
add	O
it	O
to	O
the	O
array	O
I	O
got	O
in	O
1	O
)	O

Repeat	O
to	O
get	O
the	O
bottom	O
row	O
of	O
the	O
kernel	O
[[	O
20	O
,	O
21	O
,	O
22	O
]	O
,	O
[	O
21	O
,	O
22	O
,	O
23	O
]	O
,	O
[	O
22	O
,	O
23	O
,	O
24	O
]	O
...	O
]	O
.	O

At	O
this	O
point	O
,	O
I	O
take	O
the	O
sum	O
of	O
each	O
row	O
and	O
divide	O
it	O
by	O
the	O
number	O
of	O
elements	O
in	O
the	O
filter	O
,	O
giving	O
me	O
the	O
average	O
for	O
each	O
pixel	O
,	O
(	O
shifted	O
by	O
1	O
row	O
and	O
1	O
col	O
,	O
and	O
with	O
some	O
oddities	O
around	O
edges	O
,	O
but	O
I	O
can	O
take	O
care	O
of	O
that	O
later	O
)	O
.	O

What	O
I	O
was	O
hoping	O
for	O
is	O
a	O
better	O
use	O
of	O
stride_tricks	O
to	O
get	O
the	O
9	O
values	O
or	O
the	O
sum	O
of	O
the	O
kernel	O
elements	O
directly	O
,	O
for	O
the	O
entire	O
array	O
,	O
or	O
that	O
someone	O
can	O
convince	O
me	O
of	O
another	O
more	O
efficient	O
method	O
...	O

I	O
tried	O
running	O
your	O
code	O
,	O
but	O
got	O
a	O
memory	O
corruption	O
error	O
.	O

I'm	O
running	O
Python	O
2.6.6	O
and	O
Numpy	O
1.3.0	O
on	O
Ubuntu	O
10.10	O
,	O
64-bit	O
.	O

The	O
error	O
looks	O
like	O
`	O
***	O
glibc	O
detected	O
***	O
python	O
:	O
double	O
free	O
or	O
corruption	O
(	O
!	O
prev	O
):	O
0x0000000002526d30	O
***	O
`	O
.	O

Can	O
I	O
ask	O
why	O
you	O
are	O
using	O
floats	O
(	O
I	O
presume	O
64bit	O
)	O
to	O
represent	O
an	O
image	O
that	O
can	O
be	O
(	O
probably	O
)	O
more	O
efficiently	O
stored	O
and	O
calculated	O
using	O
ints	O
?	O

Your	O
example	O
is	O
a	O
2D	O
array	O
,	O
yet	O
you	O
describe	O
your	O
data	O
as	O
3D	O
.	O

Are	O
you	O
doing	O
this	O
operation	O
for	O
each	O
of	O
16	O
layers	O
?	O

@USER	O
:	O
I	O
am	O
on	O
Python	O
2.6.6	O
.	O
and	O
Numpy	O
1.4.1	O
on	O
Windows	O
XP	O
SP3	O
.	O

I	O
have	O
no	O
idea	O
what	O
that	O
error	O
means	O
!	O

@USER	O
:	O
data	O
is	O
3D	O
(	O
an	O
image	O
with	O
16	O
channels	O
)	O
but	O
can	O
be	O
filtered	O
as	O
individual	O
layers	O
.	O

I	O
am	O
using	O
floats	O
because	O
the	O
the	O
values	O
are	O
radar	O
backscatter	O
amplitude	O
values	O
and	O
truncating	O
or	O
rescaling	O
is	O
not	O
an	O
option	O
.	O

I	O
will	O
eventually	O
need	O
to	O
use	O
Float32	O
.	O

For	O
what	O
it's	O
worth	O
,	O
here's	O
how	O
you'd	O
do	O
it	O
using	O
"	O
fancy	O
"	O
striding	O
tricks	O
.	O

I	O
was	O
going	O
to	O
post	O
this	O
yesterday	O
,	O
but	O
got	O
distracted	O
by	O
actual	O
work	O
!	O

:)	O

@USER	O
@USER	O
both	O
have	O
nice	O
implementations	O
using	O
various	O
other	O
ways	O
of	O
doing	O
this	O
.	O

Just	O
to	O
continue	O
things	O
from	O
the	O
earlier	O
question	O
,	O
I	O
figured	O
I'd	O
post	O
the	O
N-dimensional	O
equivalent	O
.	O

You're	O
not	O
going	O
to	O
be	O
able	O
to	O
significantly	O
beat	O
`	O
scipy.ndimage	O
`	O
functions	O
for	O
1D	O
arrays	O
,	O
however	O
.	O

(	O
`	O
scipy.ndimage.uniform_filter	O
`	O
should	O
beat	O
`	O
scipy.ndimage.convolve	O
`	O
,	O
though	O
)	O

Moreover	O
,	O
if	O
you're	O
trying	O
to	O
get	O
a	O
multidimensional	O
moving	O
window	O
,	O
you	O
risk	O
having	O
memory	O
usage	O
blow	O
up	O
whenever	O
you	O
inadvertently	O
make	O
a	O
copy	O
of	O
your	O
array	O
.	O

While	O
the	O
initial	O
"	O
rolling	B-API
"	O
array	O
is	O
just	O
a	O
view	O
into	O
the	O
memory	O
of	O
your	O
original	O
array	O
,	O
any	O
intermediate	O
steps	O
that	O
copy	O
the	O
array	O
will	O
make	O
a	O
copy	O
that	O
is	O
orders	O
of	O
magnitude	O
larger	O
than	O
your	O
original	O
array	O
(	O
i.e.	O
Let's	O
say	O
that	O
you're	O
working	O
with	O
a	O
100x100	O
original	O
array	O
...	O

The	O
view	O
into	O
it	O
(	O
for	O
a	O
filter	O
size	O
of	O
(	O
3	O
,	O
3	O
))	O
will	O
be	O
98x98x3x3	O
but	O
use	O
the	O
same	O
memory	O
as	O
the	O
original	O
.	O

However	O
,	O
any	O
copies	O
will	O
use	O
the	O
amount	O
of	O
memory	O
that	O
a	O
full	O
98x98x3x3	O
array	O
would	O
!!	O

)	O

Basically	O
,	O
using	O
crazy	O
striding	O
tricks	O
is	O
great	O
for	O
when	O
you	O
want	O
to	O
vectorize	O
moving	O
window	O
operations	O
on	O
a	O
single	O
axis	O
of	O
an	O
ndarray	O
.	O

It	O
makes	O
it	O
really	O
easy	O
to	O
calculate	O
things	O
like	O
a	O
moving	O
standard	O
deviation	O
,	O
etc	O
with	O
very	O
little	O
overhead	O
.	O

When	O
you	O
want	O
to	O
start	O
doing	O
this	O
along	O
multiple	O
axes	O
,	O
it's	O
possible	O
,	O
but	O
you're	O
usually	O
better	O
off	O
with	O
more	O
specialized	O
functions	O
.	O

(	O
Such	O
as	O
`	O
scipy.ndimage	O
`	O
,	O
etc	O
)	O

At	O
any	O
rate	O
,	O
here's	O
how	O
you	O
do	O
it	O
:	O

So	O
what	O
we	O
get	O
when	O
we	O
do	O
`	O
b	O
=	O
rolling_window	B-API
(	O
a	O
,	O
filtsize	O
)`	O
is	O
an	O
8x8x3x3	O
array	O
,	O
that's	O
actually	O
a	O
view	O
into	O
the	O
same	O
memory	O
as	O
the	O
original	O
10x10	O
array	O
.	O

We	O
could	O
have	O
just	O
as	O
easily	O
used	O
different	O
filter	O
size	O
along	O
different	O
axes	O
or	O
operated	O
only	O
along	O
selected	O
axes	O
of	O
an	O
N-dimensional	O
array	O
(	O
i.e.	O
`	O
filtsize	O
=	O
(	O
0	O
,	O
3	O
,	O
0	O
,	O
3	O
)`	O
on	O
a	O
4-dimensional	O
array	O
would	O
give	O
us	O
a	O
6	O
dimensional	O
view	O
)	O
.	O

We	O
can	O
then	O
apply	B-API
an	O
arbitrary	O
function	O
to	O
the	O
last	O
axis	O
repeatedly	O
to	O
effectively	O
calculate	O
things	O
in	O
a	O
moving	O
window	O
.	O

However	O
,	O
because	O
we're	O
storing	O
temporary	O
arrays	O
that	O
are	O
much	O
bigger	O
than	O
our	O
original	O
array	O
on	O
each	O
step	O
of	O
`	O
mean	B-API
`	O
(	O
or	O
`	O
std	B-API
`	O
or	O
whatever	O
)	O
,	O
this	O
is	O
not	O
at	O
all	O
memory	O
efficient	O
!	O

It's	O
also	O
not	O
going	O
to	O
be	O
terribly	O
fast	O
,	O
either	O
.	O

The	O
equivalent	O
for	O
`	O
ndimage	O
`	O
is	O
just	O
:	O

This	O
will	O
handle	O
a	O
variety	O
of	O
boundary	O
conditions	O
,	O
do	O
the	O
"	O
blurring	O
"	O
in-place	O
without	O
requiring	O
a	O
temporary	O
copy	O
of	O
the	O
array	O
,	O
and	O
be	O
very	O
fast	O
.	O

Striding	O
tricks	O
are	O
a	O
good	O
way	O
to	O
apply	B-API
a	O
function	O
to	O
a	O
moving	O
window	O
along	O
one	O
axis	O
,	O
but	O
they're	O
not	O
a	O
good	O
way	O
to	O
do	O
it	O
along	O
multiple	O
axes	O
,	O
usually	O
....	O

Just	O
my	O
$	O
0.02	O
,	O
at	O
any	O
rate	O
...	O

Very	O
well	O
put	O
:	O
`	O
Striding	O
tricks	O
are	O
a	O
good	O
way	O
to	O
apply	B-API
a	O
function	O
to	O
a	O
moving	O
window	O
along	O
one	O
axis	O
,	O
but	O
they're	O
not	O
a	O
good	O
way	O
to	O
do	O
it	O
along	O
multiple	O
axes	O
,	O
usually	O
....	O

`	O
.	O

And	O
of	O
course	O
your	O
explanation	O
of	O
the	O
memory	O
'	O
blow	O
up	O
'	O
is	O
important	O
one	O
.	O

Kind	O
of	O
summary	O
from	O
your	O
answer	O
(	O
at	O
least	O
for	O
me	O
)	O
is	O
:	O
'	O
don't	O
go	O
too	O
far	O
fishing	O
,	O
the	O
quarenteed	O
catch	O
is	O
allready	O
in	O
scipy	O
'	O
.	O

Thanks	O

Thanks	O
,	O
Joe	O
,	O
for	O
this	O
answer	O
.	O

In	O
`	O
rolling_window	B-API
`	O
should	O
the	O
`	O
if	O
not	O
hasattr	O
(	O
...	O
):	O
`	O
be	O
returning	O
`	O
rolling_window_lastaxis	O
(	O
...	O
)`	O
rather	O
than	O
`	O
rolling_window	B-API
`	O
?	O

@USER	O
-	O
Quite	O
right	O
!	O

That	O
was	O
a	O
typo	O
on	O
my	O
part	O
...	O

(	O
I	O
renamed	O
the	O
functions	O
and	O
forgot	O
to	O
change	O
that	O
part	O
of	O
it	O
.	O
)	O
Thanks	O
!	O

Is	O
it	O
possible	O
to	O
specify	O
the	O
step	O
size	O
?	O

I'm	O
not	O
familiar	O
enough	O
with	O
Python	O
to	O
write	O
out	O
code	O
for	O
that	O
,	O
but	O
the	O
two	O
best	O
ways	O
to	O
speed	O
up	O
convolutions	O
is	O
to	O
either	O
separate	O
the	O
filter	O
or	O
to	O
use	O
the	O
Fourier	O
transform	B-API
.	O

Separated	O
filter	O
:	O
Convolution	O
is	O
O	O
(	O
M*N	O
)	O
,	O
where	O
M	O
and	O
N	O
are	O
number	O
of	O
pixels	O
in	O
the	O
image	O
and	O
the	O
filter	O
,	O
respectively	O
.	O

Since	O
average	O
filtering	O
with	O
a	O
3-by-3	O
kernel	O
is	O
equivalent	O
to	O
filtering	O
first	O
with	O
a	O
3-by-1	O
kernel	O
and	O
then	O
a	O
1-by-3	O
kernel	O
,	O
you	O
can	O
get	O
`	O
(	O
3+3	O
)	O
/(	O
3*3	O
)`	O
=	O
~30%	O
speed	O
improvement	O
by	O
consecutive	O
convolution	O
with	O
two	O
1-d	O
kernels	O
(	O
this	O
obviously	O
gets	O
better	O
as	O
the	O
kernel	O
gets	O
larger	O
)	O
.	O

You	O
may	O
still	O
be	O
able	O
to	O
use	O
stride	O
tricks	O
here	O
,	O
of	O
course	O
.	O

Fourier	O
Transform	B-API
:	O
`	O
conv	O
(	O
A	O
,	O
B	O
)`	O
is	O
equivalent	O
to	O
`	O
ifft	O
(	O
fft	O
(	O
A	O
)	O
*fft	O
(	O
B	O
))`	O
,	O
i.e.	O
a	O
convolution	O
in	O
direct	O
space	O
becomes	O
a	O
multiplication	O
in	O
Fourier	O
space	O
,	O
where	O
`	O
A	O
`	O
is	O
your	O
image	O
and	O
`	O
B	O
`	O
is	O
your	O
filter	O
.	O

Since	O
the	O
(	O
element-wise	O
)	O
multiplication	O
of	O
the	O
Fourier	O
transforms	O
requires	O
that	O
A	O
and	O
B	O
are	O
the	O
same	O
size	O
,	O
B	O
is	O
an	O
array	O
of	O
`	O
size	O
(	O
A	O
)`	O
with	O
your	O
kernel	O
at	O
the	O
very	O
center	O
of	O
the	O
image	O
and	O
zeros	O
everywhere	O
else	O
.	O

To	O
place	O
a	O
3-by-3	O
kernel	O
at	O
the	O
center	O
of	O
an	O
array	O
,	O
you	O
may	O
have	O
to	O
pad	O
`	O
A	O
`	O
to	O
odd	O
size	O
.	O

Depending	O
on	O
your	O
implementation	O
of	O
the	O
Fourier	O
transform	B-API
,	O
this	O
can	O
be	O
a	O
lot	O
faster	O
than	O
the	O
convolution	O
(	O
and	O
if	O
you	O
apply	B-API
the	O
same	O
filter	O
multiple	O
times	O
,	O
you	O
can	O
pre-compute	O
`	O
fft	O
(	O
B	O
)`	O
,	O
saving	O
another	O
30%	O
of	O
computation	O
time	O
)	O
.	O

For	O
what	O
it's	O
worth	O
,	O
in	O
python	O
,	O
these	O
are	O
implemented	O
in	O
`	O
scipy.ndimage.uniform_filter	O
`	O
and	O
`	O
scipy.signal.fftconvolve	O
`	O
,	O
respectively	O
.	O

@USER	O
:	O
Cool	O
!	O

The	O
seperated	O
filter	O
approach	O
works	O
nicely	O
,	O
as	O
you	O
say	O
it	O
saves	O
more	O
time	O
as	O
the	O
kernel	O
size	O
increases	O
.	O

For	O
a	O
5000x5000	O
array	O
,	O
at	O
an	O
11x11	O
kernel	O
size	O
,	O
I	O
am	O
getting	O
7.7s	O
for	O
2d	O
convolution	O
using	O
ndimage.convolve	O
,	O
and	O
2.0s	O
for	O
two	O
1d	O
convolutions	O
using	O
ndimage.convolve1d	O
.	O

For	O
your	O
second	O
solution	O
what	O
is	O
B	O
?	O

@USER	O
:	O
I	O
have	O
expanded	O
my	O
explanation	O
of	O
the	O
second	O
solution	O

@USER	O
Kington	O
:	O
Thanks	O
!	O

If	O
I	O
understand	O
the	O
help	O
correctly	O
,	O
fftconvolve	O
doesn't	O
allow	O
you	O
to	O
precompute	O
`	O
fft	O
(	O
B	O
)`	O
,	O
right	O
?	O

@USER	O
-	O
`	O
uniform_filter	O
`	O
already	O
does	O
repeated	O
1D	O
convolutions	O
,	O
for	O
what	O
it's	O
worth	O
.	O

@USER	O
-	O
No	O
,	O
it	O
doesn't	O
...	O

It's	O
just	O
a	O
convince	O
function	O
for	O
doing	O
it	O
once	O
.	O

One	O
thing	O
I	O
am	O
confident	O
needs	O
to	O
be	O
fixed	O
is	O
your	O
view	O
array	O
`	O
b	O
`	O
.	O

It	O
has	O
a	O
few	O
items	O
from	O
unallocated	O
memory	O
,	O
so	O
you'll	O
get	O
crashes	O
.	O

Given	O
your	O
new	O
description	O
of	O
your	O
algorithm	O
,	O
the	O
first	O
thing	O
that	O
needs	O
fixing	O
is	O
the	O
fact	O
that	O
you	O
are	O
striding	O
outside	O
the	O
allocation	O
of	O
`	O
a	O
`	O
:	O

Update	O

Because	O
I'm	O
still	O
not	O
quite	O
grasping	O
the	O
method	O
and	O
there	O
seems	O
to	O
be	O
simpler	O
ways	O
to	O
solve	O
the	O
problem	O
,	O
I'm	O
just	O
going	O
to	O
put	O
this	O
here	O
:	O

...	O
which	O
just	O
seems	O
like	O
the	O
straightforward	O
approach	O
.	O

The	O
only	O
extraneous	O
operation	O
is	O
that	O
it	O
has	O
allocate	O
and	O
populate	O
`	O
B	O
`	O
only	O
once	O
.	O

All	O
the	O
addition	O
,	O
division	O
and	O
indexing	O
has	O
to	O
be	O
done	O
regardless	O
.	O

If	O
you	O
are	O
doing	O
16	O
bands	O
,	O
you	O
still	O
only	O
need	O
to	O
allocate	O
`	O
B	O
`	O
once	O
if	O
your	O
intent	O
is	O
to	O
save	O
an	O
image	O
.	O

Even	O
if	O
this	O
is	O
no	O
help	O
,	O
it	O
might	O
clarify	O
why	O
I	O
don't	O
understand	O
the	O
problem	O
,	O
or	O
at	O
least	O
serve	O
as	O
a	O
benchmark	O
to	O
time	O
the	O
speedups	O
of	O
other	O
methods	O
.	O

This	O
runs	O
in	O
2.6	O
sec	O
on	O
my	O
laptop	O
on	O
a	O
5k	O
x	O
5k	O
array	O
of	O
float64's	O
,	O
0.5	O
of	O
which	O
is	O
the	O
creation	O
of	O
`	O
B	O
`	O

I	O
just	O
timed	O
and	O
your	O
method	O
is	O
some	O
10x	O
faster	O
than	O
mine	O
.	O

The	O
ratio	O
seem	O
to	O
be	O
quite	O
constant	O
(	O
i.e.	O
it's	O
not	O
depending	O
on	O
input	O
size	O
)	O
.	O

A	O
hasty	O
conclusion	O
would	O
be	O
that	O
stride_tricks	O
are	O
usefull	O
tricks	O
sometime	O
,	O
but	O
they	O
won't	O
necessary	O
give	O
any	O
performance	O
boost	O
?	O

Alltough	O
there	O
may	O
exists	O
some	O
other	O
tricks	O
than	O
mine	O
to	O
perform	O
much	O
better	O
.	O

Thanks	O

@USER	O
:	O
For	O
a	O
5000x5000	O
array	O
and	O
a	O
3x3	O
filter	O
,	O
I	O
time	O
@USER	O
'	O
s	O
result	O
at	O
3.9s	O
,	O
@USER	O
'	O
s	O
result	O
at	O
1.9s	O
and	O
scipy.ndimage.filters.convolve	O
at	O
1.4s	O
.	O

At	O
that	O
array	O
size	O
,	O
the	O
strides	B-API
solution	O
does	O
not	O
work	O
at	O
larger	O
kernel	O
sizes	O
.	O

I'm	O
going	O
to	O
upgrade	O
@USER	O
'	O
s	O
solution	O
to	O
accept	O
variable	O
kernel	O
sizes	O
and	O
compare	O
.	O

But	O
it	O
still	O
seems	O
that	O
scipy.ndimage.filters.convolve	O
is	O
the	O
fastest	O
solution	O
...	O

Lets	O
see	O
:	O

It's	O
not	O
so	O
clear	O
form	O
your	O
question	O
,	O
but	O
I'm	O
assuming	O
now	O
that	O
you'll	O
like	O
to	O
improve	O
significantly	O
this	O
kind	O
of	O
averaging	O
.	O

Now	O
,	O
what	O
kind	O
of	O
performance	O
improvements	O
you	O
would	O
actually	O
expect	O
?	O

Update	O
:	O

First	O
of	O
all	O
,	O
a	O
warning	O
:	O
the	O
code	O
in	O
it's	O
current	O
state	O
does	O
not	O
adapt	O
properly	O
to	O
the	O
'	O
kernel	O
'	O
shape	O
.	O

However	O
that's	O
not	O
my	O
primary	O
concern	O
right	O
now	O
(	O
anyway	O
the	O
idea	O
is	O
there	O
allready	O
how	O
to	O
adapt	O
properly	O
)	O
.	O

I	O
have	O
just	O
chosen	O
the	O
new	O
shape	O
of	O
a	O
4D	O
A	O
intuitively	O
,	O
for	O
me	O
it	O
really	O
make	O
sense	O
to	O
think	O
about	O
a	O
2D	O
'	O
kernel	O
'	O
center	O
to	O
be	O
centered	O
to	O
each	O
grid	O
position	O
of	O
original	O
2D	O
A	O
.	O

But	O
that	O
4D	O
shaping	O
may	O
not	O
actually	O
be	O
the	O
'	O
best	O
'	O
one	O
.	O

I	O
think	O
the	O
real	O
problem	O
here	O
is	O
the	O
performance	O
of	O
summing	O
.	O

One	O
should	O
to	O
be	O
able	O
to	O
find	O
'	O
best	O
order	O
'	O
(	O
of	O
the	O
4D	O
A	O
)	O
inorder	O
to	O
fully	O
utilize	O
your	O
machines	O
cache	O
architecture	O
.	O

However	O
that	O
order	O
may	O
not	O
be	O
the	O
same	O
for	O
'	O
small	O
'	O
arrays	O
which	O
kind	O
of	O
'	O
co-operates	O
'	O
with	O
your	O
machines	O
cache	O
and	O
those	O
larger	O
ones	O
,	O
which	O
don't	O
(	O
at	O
least	O
not	O
so	O
straightforward	O
manner	O
)	O
.	O

Update	O
2	O
:	O

Here	O
is	O
a	O
slightly	O
modified	O
version	O
of	O
`	O
mf	O
`	O
.	O

Clearly	O
it's	O
better	O
to	O
reshape	O
to	O
a	O
3D	O
array	O
first	O
and	O
then	O
instead	O
of	O
summing	O
just	O
do	O
dot	O
product	O
(	O
this	O
has	O
the	O
advantage	O
all	O
so	O
,	O
that	O
kernel	O
can	O
be	O
arbitrary	O
)	O
.	O

However	O
it's	O
still	O
some	O
3x	O
slower	O
(	O
on	O
my	O
machine	O
)	O
than	O
Pauls	O
updated	O
function	O
.	O

@USER	O
:	O
this	O
is	O
interesting	O
.	O

I	O
see	O
you've	O
taken	O
care	O
of	O
my	O
edge	O
issues	O
,	O
although	O
your	O
filter	O
size	O
is	O
hardcoded	O
;)	O
.	O

In	O
your	O
as_strided	O
line	O
,	O
should	O
that	O
be	O
n	O
,	O
m	O
rather	O
than	O
n	O
,	O
n	O
?	O

@USER	O
:	O
I	O
modified	O
your	O
code	O
a	O
little	O
and	O
it	O
works	O
nicely	O
.	O

I	O
can't	O
wrap	O
my	O
head	O
around	O
what	O
is	O
happening	O
though	O
.	O

Could	O
you	O
describe	O
what	O
that	O
as_strided	O
line	O
is	O
doing	O
and	O
why	O
you	O
are	O
picking	O
those	O
values	O
of	O
shape	O
and	O
strides	B-API
?	O

@USER	O
:	O
shouldn't	O
one	O
of	O
those	O
n's	O
be	O
an	O
m	O
?	O

@USER	O
,	O
@USER	O
:	O
yes	O
some	O
typos	O
,	O
just	O
changed	O
those	O
in	O
my	O
answer	O
.	O

Anyway	O
I'll	O
think	O
the	O
true	O
hot	O
potatoe	O
here	O
is	O
:	O
can	O
we	O
expect	O
a	O
significant	O
improvement	O
from	O
this	O
implementation	O
?	O

I'll	O
try	O
to	O
clarify	O
my	O
answer	O
more	O
later	O
.	O

Thanks	O

@USER	O
:	O
Seems	O
to	O
be	O
having	O
issues	O
with	O
larger	O
arrays	O
(	O
like	O
5000x5000	O
)	O
,	O
even	O
at	O
a	O
3x3	O
kernel	O
size	O
...	O

