Moving	O
average	O
or	O
running	O
mean	O

Is	O
there	O
a	O
scipy	O
function	O
or	O
numpy	O
function	O
or	O
module	O
for	O
python	O
that	O
calculates	O
the	O
running	O
mean	O
of	O
a	O
1D	O
array	O
given	O
a	O
specific	O
window	O
?	O

/	O
M	O

For	O
a	O
short	O
,	O
fast	O
solution	O
that	O
does	O
the	O
whole	O
thing	O
in	O
one	O
loop	O
,	O
without	O
dependencies	O
,	O
the	O
code	O
below	O
works	O
great	O
.	O

UP	O
D:	O
more	O
efficient	O
solutions	O
have	O
been	O
proposed	O
by	O
Alleo	O
and	O
jasaarim	O
.	O

You	O
can	O
use	O
`	O
np.convolve	O
`	O
for	O
that	O
:	O

The	O
`	O
mode	B-API
`	O
argument	O
specifies	O
how	O
to	O
handle	O
the	O
edges	O
.	O

I	O
chose	O
the	O
`	O
valid	B-API
`	O
mode	O
here	O
because	O
I	O
think	O
that's	O
how	O
most	O
people	O
expect	O
running	O
mean	O
to	O
work	O
,	O
but	O
you	O
may	O
have	O
other	O
priorities	O
.	O

Here	O
is	O
a	O
plot	O
that	O
illustrates	O
the	O
difference	O
between	O
the	O
modes	O
:	O

I	O
like	O
this	O
solution	O
because	O
it	O
is	O
clean	O
(	O
one	O
line	O
)	O
and	O
relatively	O
efficient	O
(	O
work	O
done	O
inside	O
numpy	O
)	O
.	O

But	O
Alleo's	O
"	O
Efficient	O
solution	O
"	O
using	O
`	O
numpy.cumsum	O
`	O
has	O
better	O
complexity	O
.	O

Efficient	O
solution	O

Convolution	O
is	O
much	O
better	O
than	O
straightforward	O
approach	O
,	O
but	O
(	O
I	O
guess	O
)	O
it	O
uses	O
FFT	O
and	O
thus	O
quite	O
slow	O
.	O

However	O
specially	O
for	O
computing	O
the	O
running	O
mean	O
the	O
following	O
approach	O
works	O
fine	O

The	O
code	O
to	O
check	O

Note	O
that	O
`	O
numpy.allclose	O
(	O
result1	O
,	O
result2	O
)`	O
is	O
`	O
True	O
`	O
,	O
two	O
methods	O
are	O
equivalent	O
.	O

The	O
greater	O
N	O
,	O
the	O
greater	O
difference	O
in	O
time	O
.	O

Nice	O
solution	O
!	O

My	O
hunch	O
is	O
`	O
numpy.convolve	O
`	O
is	O
O	O
(	O
mn	O
);	O
its	O
docs	O
mention	O
that	O
`	O
scipy.signal.fftconvolve	O
`	O
uses	O
FFT	O
.	O

This	O
method	O
does	O
not	O
deal	O
with	O
the	O
edges	O
of	O
the	O
array	O
,	O
does	O
it	O
?	O

I	O
thought	O
that	O
calling	O
`	O
numpy.ones	O
`	O
every	O
time	O
,	O
like	O
you	O
do	O
,	O
would	O
cause	O
quite	O
some	O
overhead	O
and	O
dominate	O
the	O
comparison	O
(	O
in	O
particular	O
for	O
high	O
`	O
N	O
`)	O
,	O
but	O
that	O
seems	O
not	O
to	O
be	O
the	O
case	O
.	O

On	O
my	O
machine	O
,	O
I	O
was	O
unable	O
to	O
measure	O
any	O
difference	O
at	O
all	O
and	O
`	O
running_mean	O
`	O
is	O
still	O
orders	O
of	O
magnitude	O
faster	O
.	O

Actually	O
,	O
the	O
function	O
doest	O
work	O
for	O
python	O
3.5	O
when	O
passing	O
it	O
a	O
np	O
array	O
1-d	O

This	O
didnt	O
work	O
for	O
me	O
with	O
py3.5	O
(	O
`	O
cannot	O
perform	O
accumulate	O
with	O
flexible	O
type	O
`)	O

You	O
can	O
calculate	O
a	O
running	O
mean	O
with	O
:	O

But	O
it's	O
slow	O
.	O

Fortunately	O
,	O
numpy	O
includes	O
a	O
convolve	O
function	O
which	O
we	O
can	O
use	O
to	O
speed	O
things	O
up	O
.	O

The	O
running	O
mean	O
is	O
equivalent	O
to	O
convolving	O
`	O
x	O
`	O
with	O
a	O
vector	O
that	O
is	O
`	O
N	O
`	O
long	O
,	O
with	O
all	O
members	O
equal	O
to	O
`	O
1	O
/	O
N	O
`	O
.	O

The	O
numpy	O
implementation	O
of	O
convolve	O
includes	O
the	O
starting	O
transient	O
,	O
so	O
you	O
have	O
to	O
remove	O
the	O
first	O
N-1	O
points	O
:	O

On	O
my	O
machine	O
,	O
the	O
fast	O
version	O
is	O
20-30	O
times	O
faster	O
,	O
depending	O
on	O
the	O
length	O
of	O
the	O
input	O
vector	O
and	O
size	O
of	O
the	O
averaging	O
window	O
.	O

Note	O
that	O
convolve	O
does	O
include	O
a	O
`'	O
same	O
'`	O
mode	O
which	O
seems	O
like	O
it	O
should	O
address	O
the	O
starting	O
transient	O
issue	O
,	O
but	O
it	O
splits	O
it	O
between	O
the	O
beginning	O
and	O
end	O
.	O

Note	O
that	O
removing	O
the	O
first	O
N-1	O
points	O
still	O
leaves	O
a	O
boundary	O
effect	O
in	O
the	O
last	O
points	O
.	O

An	O
easier	O
way	O
to	O
solve	O
the	O
issue	O
is	O
to	O
use	O
`	O
mode=	O
'	O
valid	O
'`	O
in	O
`	O
convolve	O
`	O
which	O
doesn't	O
require	O
any	O
post-processing	O
.	O

@USER	O
-	O
`	O
mode=	O
'	O
valid	O
'`	O
removes	O
the	O
transient	O
from	O
both	O
ends	O
,	O
right	O
?	O

If	O
`	O
len	O
(	O
x	O
)=	O
10	O
`	O
and	O
`	O
N=4	O
`	O
,	O
for	O
a	O
running	O
mean	O
I	O
would	O
want	O
10	O
results	O
but	O
`	O
valid	B-API
`	O
returns	O
7	O
.	O

It	O
removes	O
the	O
transient	O
from	O
the	O
end	O
,	O
and	O
the	O
beginning	O
doesn't	O
have	O
one	O
.	O

Well	O
,	O
I	O
guess	O
it's	O
a	O
matter	O
of	O
priorities	O
,	O
I	O
don't	O
need	O
the	O
same	O
number	O
of	O
results	O
on	O
the	O
expense	O
of	O
getting	O
a	O
slope	O
towards	O
zero	O
that	O
isn't	O
there	O
in	O
the	O
data	O
.	O

BTW	O
,	O
here	O
is	O
a	O
command	O
to	O
show	O
the	O
difference	O
between	O
the	O
modes	O
:	O
`	O
modes	O
=	O
(	O
'	O
full	O
'	O
,	O
'	O
same	O
'	O
,	O
'	O
valid	O
')	O
;	O
[	O
plot	O
(	O
convolve	O
(	O
ones	O
((	O
200	O
,	O
))	O
,	O
ones	O
((	O
50	O
,	O
))	O
/	O
50	O
,	O
mode=m	O
))	O
for	O
m	O
in	O
modes	O
];	O
axis	O
([	O
-10	O
,	O
251	O
,	O
-	O
.1	O
,	O
1.1	O
]);	O
legend	O
(	O
modes	O
,	O
loc=	O
'	O
lower	O
center	O
')`	O
(	O
with	O
pyplot	O
and	O
numpy	O
imported	O
)	O
.	O

@USER	O
-	O
very	O
nice	O
.	O

You	O
should	O
make	O
this	O
an	O
answer	O
,	O
I	O
would	O
certainly	O
give	O
it	O
a	O
vote	O
.	O

pandas	O
is	O
more	O
suitable	O
for	O
this	O
than	O
NumPy	O
or	O
SciPy	O
.	O

Its	O
function	O
rolling_mean	B-API
does	O
the	O
job	O
conveniently	O
.	O

It	O
also	O
returns	O
a	O
NumPy	O
array	O
when	O
the	O
input	O
is	O
an	O
array	O
.	O

It	O
is	O
difficult	O
to	O
beat	O
`	O
rolling_mean	B-API
`	O
in	O
performance	O
with	O
any	O
custom	O
pure	O
Python	O
implementation	O
.	O

Here	O
is	O
an	O
example	O
performance	O
against	O
two	O
of	O
the	O
proposed	O
solutions	O
:	O

There	O
are	O
also	O
nice	O
options	O
as	O
to	O
how	O
to	O
deal	O
with	O
the	O
edge	O
values	O
.	O

The	O
Pandas	O
rolling_mean	B-API
is	O
a	O
nice	O
tool	O
for	O
the	O
job	O
but	O
has	O
been	O
deprecated	O
for	O
ndarrays	O
.	O

In	O
future	O
Pandas	O
releases	O
it	O
will	O
only	O
function	O
on	O
Pandas	O
series	O
.	O

Where	O
do	O
we	O
turn	O
now	O
for	O
non-Pandas	O
array	O
data	O
?	O

@USER	O
rolling_mean()	B-API
is	O
deprecated	O
,	O
but	O
now	O
you	O
can	O
use	O
rolling	B-API
and	O
mean	O
separately	O
:	O
`	O
df.rolling	O
(	O
windowsize	O
)	O
.mean()	B-API
`	O
now	O
works	O
instead	O
(	O
very	O
quickly	O
I	O
might	O
add	O
)	O
.	O
for	O
6,000	O
row	O
series	O
`	O
%timeit	O
test1.rolling	O
(	O
20	O
)	O
.mean()	B-API
`	O
returned	O
1000	O
loops	O
,	O
best	O
of	O
3	O
:	O
1.16	O
ms	O
per	O
loop	O

@USER	O
`	O
df.rolling()	O
`	O
works	O
well	O
enough	O
,	O
the	O
problem	O
is	O
that	O
even	O
this	O
form	O
will	O
not	O
support	O
ndarrays	O
in	O
the	O
future	O
.	O

To	O
use	O
it	O
we	O
will	O
have	O
to	O
load	O
our	O
data	O
into	O
a	O
Pandas	O
Dataframe	B-API
first	O
.	O

I	O
would	O
love	O
to	O
see	O
this	O
function	O
added	O
to	O
either	O
`	O
numpy	O
`	O
or	O
`	O
scipy.signal	O
`	O
.	O

@USER	O
totally	O
agree	O
.	O

I	O
am	O
struggling	O
in	O
particular	O
to	O
match	O
pandas	O
.ewm()	O
.mean()	B-API
speed	O
for	O
my	O
own	O
arrays	O
(	O
instead	O
of	O
having	O
to	O
load	O
them	O
into	O
a	O
df	O
first	O
)	O
.	O

I	O
mean	O
,	O
it's	O
great	O
that	O
it's	O
fast	O
,	O
but	O
just	O
feels	O
a	O
bit	O
clunky	O
moving	O
in	O
and	O
out	O
of	O
dataframes	O
too	O
often	O
.	O

`	O
%timeit	O
bottleneck.move_mean	O
(	O
x	O
,	O
N	O
)`	O
is	O
3	O
to	O
15	O
times	O
faster	O
than	O
the	O
cumsum	B-API
and	O
pandas	O
methods	O
on	O
my	O
pc	O
.	O

Take	O
a	O
look	O
at	O
their	O
benchmark	O
in	O
the	O
repo's	O
README	O
.	O

For	O
a	O
ready-to-use	O
solution	O
,	O
see	O
http://www.scipy.org/Cookbook/SignalSmooth	O
.	O

It	O
provides	O
running	O
average	O
with	O
the	O
`	O
flat	O
`	O
window	O
type	O
.	O

Note	O
that	O
this	O
is	O
a	O
bit	O
more	O
sophisticated	O
than	O
the	O
simple	O
do-it-yourself	O
convolve-method	O
,	O
since	O
it	O
tries	O
to	O
handle	O
the	O
problems	O
at	O
the	O
beginning	O
and	O
the	O
end	O
of	O
the	O
data	O
by	O
reflecting	O
it	O
(	O
which	O
may	O
or	O
may	O
not	O
work	O
in	O
your	O
case	O
...	O
)	O
.	O

To	O
start	O
with	O
,	O
you	O
could	O
try	O
:	O

This	O
method	O
relies	O
on	O
`	O
numpy.convolve	O
`	O
,	O
the	O
difference	O
only	O
in	O
altering	O
the	O
sequence	O
.	O

I'm	O
always	O
annoyed	O
by	O
signal	O
processing	O
function	O
that	O
return	O
output	O
signals	O
of	O
different	O
shape	O
than	O
the	O
input	O
signals	O
when	O
both	O
inputs	O
and	O
outputs	O
are	O
of	O
the	O
same	O
nature	O
(	O
e.g.	O
,	O
both	O
temporal	O
signals	O
)	O
.	O

It	O
breaks	O
the	O
correspondence	O
with	O
related	O
independent	O
variable	O
(	O
e.g.	O
,	O
time	O
,	O
frequency	O
)	O
making	O
plotting	O
or	O
comparison	O
not	O
a	O
direct	O
matter	O
...	O
anyway	O
,	O
if	O
you	O
share	O
the	O
feeling	O
,	O
you	O
might	O
want	O
to	O
change	O
the	O
last	O
lines	O
of	O
the	O
proposed	O
function	O
as	O
y=	O
np.convolve	O
(	O
w	O
/	O
w.sum()	O
,	O
s	O
,	O
mode=	O
'	O
same	O
')	O
;	O
return	O
y	O
[	O
window_len-1	O
:-(	O
window_len-1	O
)]	O

or	O
module	O
for	O
python	O
that	O
calculates	O

in	O
my	O
tests	O
at	O
Tradewave.net	O
TA-lib	O
always	O
wins	O
:	O

results	O
:	O

I	O
know	O
this	O
is	O
an	O
old	O
question	O
,	O
but	O
here	O
is	O
a	O
solution	O
that	O
doesn't	O
use	O
any	O
extra	O
data	O
structures	O
or	O
libraries	O
.	O

It	O
is	O
linear	O
in	O
the	O
number	O
of	O
elements	O
of	O
the	O
input	O
list	O
and	O
I	O
cannot	O
think	O
of	O
any	O
other	O
way	O
to	O
make	O
it	O
more	O
efficient	O
(	O
actually	O
if	O
anyone	O
knows	O
of	O
a	O
better	O
way	O
to	O
allocate	O
the	O
result	O
,	O
please	O
let	O
me	O
know	O
)	O
.	O

NOTE	O
:	O
this	O
would	O
be	O
much	O
faster	O
using	O
a	O
numpy	O
array	O
instead	O
of	O
a	O
list	O
,	O
but	O
I	O
wanted	O
to	O
eliminate	O
all	O
dependencies	O
.	O

It	O
would	O
also	O
be	O
possible	O
to	O
improve	O
performance	O
by	O
multi-threaded	O
execution	O

The	O
function	O
assumes	O
that	O
the	O
input	O
list	O
is	O
one	O
dimensional	O
,	O
so	O
be	O
careful	O
.	O

I	O
haven't	O
yet	O
checked	O
how	O
fast	O
this	O
is	O
,	O
but	O
you	O
could	O
try	O
:	O

This	O
is	O
what	O
I	O
was	O
going	O
to	O
do	O
.	O

Can	O
anyone	O
please	O
critique	O
why	O
this	O
is	O
a	O
bad	O
way	O
to	O
go	O
?	O

This	O
simple	O
python	O
solution	O
worked	O
well	O
for	O
me	O
without	O
requiring	O
numpy	O
.	O

I	O
ended	O
up	O
rolling	B-API
it	O
into	O
a	O
class	O
for	O
re-use	O
.	O

A	O
bit	O
late	O
to	O
the	O
party	O
,	O
but	O
I've	O
made	O
my	O
own	O
little	O
function	O
that	O
does	O
NOT	O
wrap	O
around	O
the	O
ends	O
or	O
pads	O
with	O
zeroes	O
that	O
are	O
then	O
used	O
to	O
find	O
the	O
average	O
as	O
well	O
.	O

As	O
a	O
further	O
treat	O
is	O
,	O
that	O
it	O
also	O
re-samples	O
the	O
signal	O
at	O
linearly	O
spaced	O
points	O
.	O

Customize	O
the	O
code	O
at	O
will	O
to	O
get	O
other	O
features	O
.	O

The	O
method	O
is	O
a	O
simple	O
matrix	O
multiplication	O
with	O
a	O
normalized	O
Gaussian	O
kernel	O
.	O

A	O
simple	O
usage	O
on	O
a	O
sinusoidal	O
signal	O
with	O
added	O
normal	O
distributed	O
noise	O
:	O

Another	O
approach	O
to	O
find	O
moving	O
average	O
without	O
using	O
numpy	O
,	O
panda	O

will	O
print	O
[	O
2.0	O
,	O
4.0	O
,	O
6.0	O
,	O
6.5	O
,	O
7.4	O
,	O
7.833333333333333	O
]	O

itertools.accumulate	O
does	O
not	O
exist	O
in	O
python	O
2.7	O
,	O
but	O
does	O
in	O
python	O
3.4	O

This	O
question	O
is	O
now	O
even	O
older	O
than	O
when	O
NeXuS	O
wrote	O
about	O
it	O
last	O
month	O
,	O
BUT	O
I	O
like	O
how	O
his	O
code	O
deals	O
with	O
edge	O
cases	O
.	O

However	O
,	O
because	O
it	O
is	O
a	O
"	O
simple	O
moving	O
average	O
,	O
"	O
it's	O
results	O
lag	O
behind	O
the	O
data	O
they	O
apply	B-API
to	O
.	O

I	O
thought	O
that	O
dealing	O
with	O
edge	O
cases	O
in	O
a	O
more	O
satisfying	O
way	O
than	O
NumPy's	O
modes	O
`	O
valid	B-API
`	O
,	O
`	O
same	O
`	O
,	O
and	O
`	O
full	O
`	O
could	O
be	O
achieved	O
by	O
applying	O
a	O
similar	O
approach	O
to	O
a	O
`	O
convolution()	O
`	O
based	O
method	O
.	O

My	O
contribution	O
uses	O
a	O
central	O
running	O
average	O
to	O
align	B-API
its	O
results	O
with	O
their	O
data	O
.	O

When	O
there	O
are	O
two	O
few	O
points	O
available	O
for	O
the	O
full-sized	O
window	O
to	O
be	O
used	O
,	O
running	O
averages	O
are	O
computed	O
from	O
successively	O
smaller	O
windows	O
at	O
the	O
edges	O
of	O
the	O
array	O
.	O

[	O
Actually	O
,	O
from	O
successively	O
larger	O
windows	O
,	O
but	O
that's	O
an	O
implementation	O
detail	O
.	O
]	O

It's	O
relatively	O
slow	O
because	O
it	O
uses	O
`	O
convolve()	O
`	O
,	O
and	O
could	O
likely	O
be	O
spruced	O
up	O
quite	O
a	O
lot	O
by	O
a	O
true	O
Pythonista	O
,	O
however	O
,	O
I	O
believe	O
that	O
the	O
idea	O
stands	O
.	O

If	O
it	O
is	O
important	O
to	O
keep	O
the	O
dimensions	O
of	O
the	O
input	O
(	O
instead	O
of	O
restricting	O
the	O
output	O
to	O
the	O
`'	O
valid	O
'`	O
area	O
of	O
a	O
convolution	O
)	O
,	O
you	O
can	O
use	O
scipy.ndimage.filters.uniform_filter1d	O
:	O

`	O
uniform_filter1d	O
`	O
allows	O
multiple	O
ways	O
to	O
handle	O
the	O
border	O
where	O
`'	O
reflect	O
'`	O
is	O
the	O
default	O
,	O
but	O
in	O
my	O
case	O
,	O
I	O
rather	O
wanted	O
`'	O
nearest	O
'`	O
.	O

It	O
is	O
also	O
rather	O
quick	O
(	O
nearly	O
50	O
times	O
faster	O
than	O
`	O
np.convolve	O
`)	O
:	O

If	O
you	O
do	O
choose	O
to	O
roll	O
your	O
own	O
,	O
rather	O
than	O
use	O
an	O
existing	O
library	O
,	O
please	O
be	O
conscious	O
of	O
floating	O
point	O
error	O
and	O
try	O
to	O
minimize	O
its	O
effects	O
:	O

If	O
all	O
your	O
values	O
are	O
roughly	O
the	O
same	O
order	O
of	O
magnitude	O
,	O
then	O
this	O
will	O
help	O
to	O
preserve	O
precision	O
by	O
always	O
adding	O
values	O
of	O
roughly	O
similar	O
magnitudes	O
.	O

This	O
is	O
a	O
terribly	O
unclear	O
answer	O
,	O
at	O
least	O
some	O
comment	O
in	O
the	O
code	O
or	O
explanation	O
of	O
why	O
this	O
helps	O
floating	O
point	O
error	O
would	O
be	O
nice	O
.	O

In	O
my	O
last	O
sentence	O
I	O
was	O
trying	O
to	O
indicate	O
why	O
it	O
helps	O
floating	O
point	O
error	O
.	O

If	O
two	O
values	O
are	O
approximately	O
the	O
same	O
order	O
of	O
magnitude	O
,	O
then	O
adding	O
them	O
loses	O
less	O
precision	O
than	O
if	O
you	O
added	O
a	O
very	O
large	O
number	O
to	O
a	O
very	O
small	O
one	O
.	O

The	O
code	O
combines	O
"	O
adjacent	O
"	O
values	O
in	O
a	O
manner	O
that	O
even	O
intermediate	O
sums	O
should	O
always	O
be	O
reasonably	O
close	O
in	O
magnitude	O
,	O
to	O
minimize	O
the	O
floating	O
point	O
error	O
.	O

Nothing	O
is	O
fool	O
proof	O
but	O
this	O
method	O
has	O
saved	O
a	O
couple	O
very	O
poorly	O
implemented	O
projects	O
in	O
production	O
.	O

1	O
.	O
being	O
applied	O
to	O
original	O
problem	O
,	O
this	O
would	O
be	O
terribly	O
slow	O
(	O
computing	O
average	O
)	O
,	O
so	O
this	O
is	O
just	O
irrelevant	O
2	O
.	O
to	O
suffer	O
from	O
the	O
problem	O
of	O
precision	O
of	O
64-bit	O
numbers	O
,	O
one	O
has	O
to	O
sum	O
up	O
2^30	O
of	O
nearly	O
equal	O
numbers	O
.	O

@USER	O
:	O
Instead	O
of	O
doing	O
one	O
addition	O
per	O
value	O
,	O
you'll	O
be	O
doing	O
two	O
.	O

The	O
proof	O
is	O
the	O
same	O
as	O
the	O
bit-flipping	O
problem	O
.	O

However	O
,	O
the	O
point	O
of	O
this	O
answer	O
is	O
not	O
necessarily	O
performance	O
,	O
but	O
precision	O
.	O

Memory	O
usage	O
for	O
averaging	O
64-bit	O
values	O
would	O
not	O
exceed	O
64	O
elements	O
in	O
the	O
cache	O
,	O
so	O
it's	O
friendly	O
in	O
memory	O
usage	O
as	O
well	O
.	O

Yes	O
,	O
you're	O
right	O
that	O
this	O
takes	O
2x	O
more	O
operations	O
than	O
simple	O
sum	O
,	O
but	O
the	O
original	O
problem	O
is	O
compute	O
running	O
mean	O
,	O
not	O
just	O
sum	O
.	O

Which	O
can	O
be	O
done	O
in	O
O	O
(	O
n	O
)	O
,	O
but	O
your	O
answer	O
requires	O
O	O
(	O
mn	O
)	O
,	O
where	O
m	O
is	O
size	O
of	O
window	O
.	O

